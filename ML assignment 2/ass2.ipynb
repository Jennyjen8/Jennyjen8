{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id  num_critic_for_reviews  duration  director_facebook_likes  \\\n",
      "0      1                      27       118                       14   \n",
      "1      2                     339       141                        0   \n",
      "2      3                      78        95                       89   \n",
      "3      4                     226       117                        0   \n",
      "4      5                      97       104                       38   \n",
      "..   ...                     ...       ...                      ...   \n",
      "747  748                     179        93                        0   \n",
      "748  749                     393       105                      335   \n",
      "749  750                      55       117                      133   \n",
      "750  751                      85        72                        0   \n",
      "751  752                      35        87                        4   \n",
      "\n",
      "     actor_3_facebook_likes  actor_1_facebook_likes      gross  \\\n",
      "0                       400                    2000    2246000   \n",
      "1                       404                     749   47307550   \n",
      "2                       388                     963      37606   \n",
      "3                       818                   15000  104054514   \n",
      "4                       690                     801    3447339   \n",
      "..                      ...                     ...        ...   \n",
      "747                     766                   13000   17096053   \n",
      "748                     911                    3000   37516013   \n",
      "749                     249                     687   20966644   \n",
      "750                     384                    3000   47887943   \n",
      "751                     624                    2000      19539   \n",
      "\n",
      "     num_voted_users  cast_total_facebook_likes  facenumber_in_poster  ...  \\\n",
      "0               2302                       3384                     4  ...   \n",
      "1             104301                       1948                     4  ...   \n",
      "2              31836                       2658                     0  ...   \n",
      "3             200359                      16828                     0  ...   \n",
      "4              29517                       2667                     7  ...   \n",
      "..               ...                        ...                   ...  ...   \n",
      "747           134458                      15716                     2  ...   \n",
      "748           128629                       8281                     0  ...   \n",
      "749            29610                       1665                     0  ...   \n",
      "750            11634                       4480                     0  ...   \n",
      "751             4182                       6227                     1  ...   \n",
      "\n",
      "     actor_3_name_Willie Garson  actor_3_name_Willie Nelson  \\\n",
      "0                           0.0                         0.0   \n",
      "1                           0.0                         0.0   \n",
      "2                           0.0                         0.0   \n",
      "3                           0.0                         0.0   \n",
      "4                           0.0                         0.0   \n",
      "..                          ...                         ...   \n",
      "747                         0.0                         0.0   \n",
      "748                         0.0                         0.0   \n",
      "749                         0.0                         0.0   \n",
      "750                         0.0                         0.0   \n",
      "751                         0.0                         0.0   \n",
      "\n",
      "     actor_3_name_Xander Berkeley  actor_3_name_Yuria Nara  \\\n",
      "0                             0.0                      0.0   \n",
      "1                             0.0                      0.0   \n",
      "2                             0.0                      0.0   \n",
      "3                             0.0                      0.0   \n",
      "4                             0.0                      0.0   \n",
      "..                            ...                      ...   \n",
      "747                           0.0                      0.0   \n",
      "748                           0.0                      0.0   \n",
      "749                           0.0                      0.0   \n",
      "750                           0.0                      0.0   \n",
      "751                           0.0                      0.0   \n",
      "\n",
      "     actor_3_name_YÃ»suke Iseya  actor_3_name_Zach Woods  \\\n",
      "0                          0.0                      0.0   \n",
      "1                          0.0                      0.0   \n",
      "2                          0.0                      0.0   \n",
      "3                          0.0                      0.0   \n",
      "4                          0.0                      0.0   \n",
      "..                         ...                      ...   \n",
      "747                        0.0                      0.0   \n",
      "748                        0.0                      0.0   \n",
      "749                        0.0                      0.0   \n",
      "750                        0.0                      0.0   \n",
      "751                        0.0                      0.0   \n",
      "\n",
      "     actor_3_name_Zack Ward  actor_3_name_Zena Grey  \\\n",
      "0                       0.0                     0.0   \n",
      "1                       0.0                     0.0   \n",
      "2                       0.0                     0.0   \n",
      "3                       0.0                     0.0   \n",
      "4                       0.0                     0.0   \n",
      "..                      ...                     ...   \n",
      "747                     0.0                     0.0   \n",
      "748                     0.0                     0.0   \n",
      "749                     0.0                     0.0   \n",
      "750                     0.0                     0.0   \n",
      "751                     0.0                     0.0   \n",
      "\n",
      "     actor_3_name_Zhanna Friske  actor_3_name_Zhengyong Zhang  \n",
      "0                           0.0                           0.0  \n",
      "1                           0.0                           0.0  \n",
      "2                           0.0                           0.0  \n",
      "3                           0.0                           0.0  \n",
      "4                           0.0                           0.0  \n",
      "..                          ...                           ...  \n",
      "747                         0.0                           0.0  \n",
      "748                         0.0                           0.0  \n",
      "749                         0.0                           0.0  \n",
      "750                         0.0                           0.0  \n",
      "751                         0.0                           0.0  \n",
      "\n",
      "[752 rows x 1486 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import zipfile\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# train dataset\n",
    "df_train = pd.read_csv('train_dataset.csv')\n",
    "categorical_cols = ['language', 'country', 'content_rating', 'movie_title', 'actor_3_name']\n",
    "encoded_df = pd.DataFrame(encoder.fit_transform(df_train[categorical_cols]).toarray(), columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "# Concatenate the encoded columns with the original DataFrame\n",
    "df_encoded = pd.concat([df_train, encoded_df], axis=1)\n",
    "\n",
    "# Drop the original categorical columns\n",
    "df_encoded.drop(categorical_cols, axis=1, inplace=True)\n",
    "drop = ['actor_1_name', 'actor_2_name', 'genres', 'plot_keywords', 'title_embedding', 'director_name']\n",
    "df_encoded.drop(drop, axis=1, inplace=True)\n",
    "\n",
    "X_train_1 = df_encoded.iloc[:, :-1]\n",
    "y_train_1 = df_encoded['imdb_score_binned']\n",
    "\n",
    "# test dataset \n",
    "df_test = pd.read_csv('test_dataset.csv')\n",
    "encoded_df_test = pd.DataFrame(encoder.fit_transform(df_test[categorical_cols]).toarray(), columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "# Concatenate the encoded columns with the original DataFrame\n",
    "df_encoded_test= pd.concat([df_test, encoded_df_test], axis=1)\n",
    "\n",
    "# Drop the original categorical columns\n",
    "df_encoded_test.drop(categorical_cols, axis=1, inplace=True)\n",
    "df_encoded_test.drop(drop, axis=1, inplace=True)\n",
    "X_test = df_encoded_test.iloc[:, :-1]\n",
    "\n",
    "# X_train, X_test, y_train, y_test_1 = train_test_split(df_encoded.iloc[:,:-1], df_encoded.iloc[:,-1], test_size=0.2, random_state=42)\n",
    "print(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3004, 12637)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_train_actor_1_name = 'features_countvec/train_countvec_features_actor_1_name.npy'\n",
    "zip_train_actor_2_name = 'features_countvec/train_countvec_features_actor_2_name.npy'\n",
    "zip_train_director_name = 'features_countvec/train_countvec_features_director_name.npy'\n",
    "zip_train_genres = 'features_doc2vec/train_doc2vec_features_genre.npy'\n",
    "zip_train_plot_keywords = 'features_doc2vec/train_doc2vec_features_plot_keywords.npy'\n",
    "zip_train_title_embedding = 'features_fasttext/train_fasttext_title_embeddings.npy'\n",
    "\n",
    "data_director = np.load(zip_train_director_name)\n",
    "data_frame_director = pd.DataFrame(data_director)\n",
    "data_frame_pre_director = data_frame_director.add_prefix('director_name_')\n",
    "X_train_1 = pd.concat([X_train_1, data_frame_pre_director], axis=1)\n",
    "# X_train = pd.concat([X_train, data_frame_pre_director], axis=1)\n",
    "\n",
    "data_actor2 = np.load(zip_train_actor_2_name)\n",
    "data_frame_actor2 = pd.DataFrame(data_actor2)\n",
    "data_frame_pre_actor2 = data_frame_actor2.add_prefix('actor_2_name_')\n",
    "X_train_1 = pd.concat([X_train_1, data_frame_pre_actor2], axis=1)\n",
    "# X_train = pd.concat([X_train, data_frame_pre_actor2], axis=1)\n",
    "\n",
    "data_actor1 = np.load(zip_train_actor_1_name)\n",
    "data_frame_actor1 = pd.DataFrame(data_actor1)\n",
    "data_frame_pre_actor1 = data_frame_actor1.add_prefix('actor_1_name_')\n",
    "X_train_1 = pd.concat([X_train_1, data_frame_pre_actor1], axis=1)\n",
    "# X_train = pd.concat([X_train, data_frame_pre_actor1], axis=1)\n",
    "\n",
    "genre = np.load(zip_train_genres)\n",
    "data_frame_genre = pd.DataFrame(genre)\n",
    "data_frame_pre_genres = data_frame_genre.add_prefix('genres_')\n",
    "X_train_1 = pd.concat([X_train_1, data_frame_pre_genres], axis=1)\n",
    "# X_train = pd.concat([X_train, data_frame_pre_genres], axis=1)\n",
    "\n",
    "plot_keywords = np.load(zip_train_plot_keywords)\n",
    "data_plot_keywords = pd.DataFrame(plot_keywords)\n",
    "data_frame_plot_key = data_plot_keywords.add_prefix('plot_keywords_')\n",
    "X_train_1 = pd.concat([X_train_1, data_frame_plot_key], axis=1)\n",
    "# X_train = pd.concat([X_train, data_frame_plot_key], axis=1)\n",
    "\n",
    "title_embedding = np.load(zip_train_title_embedding)\n",
    "data_title_embedding = pd.DataFrame(title_embedding)\n",
    "data_frame_pre_title_embedding = data_title_embedding.add_prefix('title_embedding_')\n",
    "X_train_1 = pd.concat([X_train_1, data_frame_pre_title_embedding], axis=1)\n",
    "# X_train = pd.concat([X_train, data_frame_plot_key], axis=1)\n",
    "\n",
    "# X_train, X_test, y_train, y_test_1 = train_test_split(X_train_1, y_train_1, test_size=0.2, random_state=42)\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "# X_train_1.columns\n",
    "np.shape(X_train_1)\n",
    "# X_train_1\n",
    "# print(X_train)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "zip_train_actor_1_name = 'features_countvec/train_countvec_features_actor_1_name.npy'\n",
    "zip_train_actor_2_name = 'features_countvec/train_countvec_features_actor_2_name.npy'\n",
    "zip_train_director_name = 'features_countvec/train_countvec_features_director_name.npy'\n",
    "zip_train_genres = 'features_doc2vec/train_doc2vec_features_genre.npy'\n",
    "zip_train_plot_keywords = 'features_doc2vec/train_doc2vec_features_plot_keywords.npy'\n",
    "\n",
    "data_director = np.load(zip_train_director_name)\n",
    "\n",
    "print(data_director)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(752, 1486)\n",
      "(752, 3599)\n",
      "(752, 6518)\n",
      "(752, 8581)\n",
      "(752, 8681)\n",
      "(752, 8781)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(752, 8881)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.shape(X_test))\n",
    "\n",
    "zip_test_actor_1_name = 'features_countvec/test_countvec_features_actor_1_name.npy'\n",
    "zip_test_actor_2_name = 'features_countvec/test_countvec_features_actor_2_name.npy'\n",
    "zip_test_director_name = 'features_countvec/test_countvec_features_director_name.npy'\n",
    "zip_test_genre = 'features_doc2vec/test_doc2vec_features_genre.npy'\n",
    "zip_test_plot_keywords = 'features_doc2vec/test_doc2vec_features_plot_keywords.npy'\n",
    "zip_test_title_embedding = 'features_fasttext/test_fasttext_title_embeddings.npy'\n",
    "\n",
    "data_director = np.load(zip_test_director_name)\n",
    "data_frame_director = pd.DataFrame(data_director)\n",
    "data_frame_pre_director = data_frame_director.add_prefix('director_name_')\n",
    "X_test = pd.concat([X_test, data_frame_pre_director], axis=1)\n",
    "print(np.shape(X_test))\n",
    "\n",
    "data_actor2 = np.load(zip_test_actor_2_name)\n",
    "data_frame_actor2 = pd.DataFrame(data_actor2)\n",
    "data_frame_pre_actor2 = data_frame_actor2.add_prefix('actor_2_name_')\n",
    "X_test = pd.concat([X_test, data_frame_pre_actor2], axis=1)\n",
    "print(np.shape(X_test))\n",
    "\n",
    "data_actor1 = np.load(zip_test_actor_1_name)\n",
    "data_frame_actor1 = pd.DataFrame(data_actor1)\n",
    "data_frame_pre_actor1 = data_frame_actor1.add_prefix('actor_1_name_')\n",
    "X_test = pd.concat([X_test, data_frame_pre_actor1], axis=1)\n",
    "print(np.shape(X_test))\n",
    "\n",
    "genre = np.load(zip_test_genre)\n",
    "data_genre = pd.DataFrame(genre)\n",
    "data_frame_genre = data_genre.add_prefix('genres_')\n",
    "X_test = pd.concat([X_test, data_frame_genre], axis=1)\n",
    "print(np.shape(X_test))\n",
    "\n",
    "plot_keywords = np.load(zip_test_plot_keywords)\n",
    "data_plot_keywords = pd.DataFrame(plot_keywords)\n",
    "data_frame_plot_keywords = data_plot_keywords.add_prefix('plot_keywords_')\n",
    "X_test = pd.concat([X_test, data_frame_plot_keywords], axis=1)\n",
    "print(np.shape(X_test))\n",
    "\n",
    "title_embedding = np.load(zip_test_title_embedding)\n",
    "data_title_embedding = pd.DataFrame(title_embedding)\n",
    "data_frame_title_embedding = data_title_embedding.add_prefix('title_embedding_')\n",
    "X_test = pd.concat([X_test, data_frame_title_embedding], axis=1)\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "# print(np.shape(data_frame_director))\n",
    "# print(np.shape(data_frame_actor2))\n",
    "# print(np.shape(data_frame_actor1))\n",
    "# print(np.shape(data_frame_genre))\n",
    "# print(np.shape(data_frame_plot_keywords))\n",
    "np.shape(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(y_train_1)\n",
    "# np.shape(y_train_1)\n",
    "# zip_y_train_titleembed = 'features_fasttext/train_fasttext_title_embeddings.npy'\n",
    "\n",
    "# # title_embedding = np.load(zip_y_train_titleembed)\n",
    "# # data_titleemb = pd.DataFrame(title_embedding)\n",
    "# # data_frame_titleemb = data_genre.add_prefix('title_embedding_')\n",
    "# # y_train_1 = pd.concat([y_train_1, data_frame_titleemb], axis=1)\n",
    "# # pd.set_option('display.max_rows', None)\n",
    "# # # print(y_train_1)\n",
    "# # # np.shape(y_train_1)\n",
    "# # Check for NaN values\n",
    "# # if y_train_1.isna().any().any():\n",
    "# #     print(\"DataFrame contains NaN values\")\n",
    "# # else:\n",
    "# #     print(\"DataFrame doesn't contain NaN values\")\n",
    "\n",
    "# np.shape(y_train_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(752, 8881)\n"
     ]
    }
   ],
   "source": [
    "# compare train and test data and make columns for test data \n",
    "# Get the columns present in X_train_1 but not in X_test\n",
    "print(np.shape(X_test))\n",
    "missing_columns_X_test = list(set(X_train_1.columns) - set(X_test.columns))\n",
    "\n",
    "# Create a DataFrame with missing columns filled with 0\n",
    "missing_columns_df_X_test = [col for col in X_train_1.columns if col in missing_columns_X_test]\n",
    "missing_columns_df_X_test = pd.DataFrame(0, index=X_test.index, columns=missing_columns_X_test)\n",
    "# missing_columns_df_X_test = [col for col in X_train_1.columns if col in missing_columns_df_X_test]\n",
    "\n",
    "\n",
    "# Concatenate missing_columns_df_X_test with X_test\n",
    "X_test = pd.concat([X_test, missing_columns_df_X_test], axis=1)\n",
    "X_test = X_test[X_train_1.columns]\n",
    "# print(np.shape(X_train_1))\n",
    "# print(np.shape(X_test))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3004, 12637)\n",
      "(752, 12637)\n"
     ]
    }
   ],
   "source": [
    "# compare train and test data and make columns for train data \n",
    "missing_columns_X_train = list(set(X_test.columns) - set(X_train_1.columns))\n",
    "\n",
    "#create a Dataframe with missing columns filled with 0 \n",
    "missing_columns_df_X_train = [col for col in X_test.columns if col in missing_columns_X_train]\n",
    "missing_columns_df_X_train = pd.DataFrame(0, index=X_train_1.index, columns=missing_columns_X_train)\n",
    "# missing_columns_df_X_train = [col for col in X_test.columns if col in missing_columns_df_X_train]\n",
    "\n",
    "\n",
    "#Concatenate missing_columns_df_X_train with X_train_1\n",
    "X_train_1 = pd.concat([X_train_1, missing_columns_df_X_train], axis=1)\n",
    "X_train_1 = X_train_1[X_test.columns]\n",
    "print(np.shape(X_train_1))\n",
    "print(np.shape(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise (Standardise) the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame doesn't contain NaN values\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# def standardise_df(df):\n",
    "#     mean = df.mean(axis = 0)\n",
    "#     std_vals = df.std(axis = 0)\n",
    "    \n",
    "#     standardised_df = (df-mean)/std_vals\n",
    "\n",
    "#     return standardised_df\n",
    "\n",
    "\n",
    "# train the model by normaling dataset with standardisation \n",
    "# X_test_stan = standardise_df(X_test)\n",
    "# X_train_stan = standardise_df(X_train_1)\n",
    "# y_train_stan = standardise_df(y_train_1)\n",
    "\n",
    "# X_train, X_test, y_train, y_test_1 = train_test_split(df_train.iloc[:,:-1], df_train.iloc[:,-1], test_size=0.2, random_state=42)\n",
    "\n",
    "# Select columns to standardize\n",
    "columns_to_standardize = [col for col in X_train_1.columns if col not in categorical_cols]\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardize selected columns\n",
    "X_train_stan = X_train_1.copy()  # Create a copy of the original DataFrame\n",
    "X_train_stan[columns_to_standardize] = scaler.fit_transform(X_train_stan[columns_to_standardize])\n",
    "\n",
    "# #delete \n",
    "# columns_to_standardize1 = [col for col in X_train.columns if col not in categorical_cols]\n",
    "# X_train_stan_2 = X_train.copy()  # Create a copy of the original DataFrame\n",
    "# X_train_stan_2[columns_to_standardize] = scaler.fit_transform(X_train_stan_2[columns_to_standardize])\n",
    "\n",
    "\n",
    "# Select columns to standardize\n",
    "columns_to_standardize = [col for col in X_test.columns if col not in categorical_cols]\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardize selected columns\n",
    "X_test_stan = X_test.copy()  # Create a copy of the original DataFrame\n",
    "X_test_stan[columns_to_standardize] = scaler.fit_transform(X_test_stan[columns_to_standardize])\n",
    "\n",
    "#delete\n",
    "# X_test_stan_2 = X_test_1.copy()  # Create a copy of the original DataFrame\n",
    "# # X_test_stan_2[columns_to_standardize] = scaler.fit_transform(X_test_stan_2[columns_to_standardize])\n",
    "\n",
    "# for column in columns_to_standardize:\n",
    "#     # X_test_stan_2[column] = scaler.fit_transform(X_test_stan_2[column])\n",
    "#     print(column)\n",
    "\n",
    "\n",
    "# Print the standardized DataFrame\n",
    "# print(X_train_stan)\n",
    "X_train, X_test_1, y_train, y_test_1 = train_test_split(X_train_stan, y_train_1, test_size=0.2, random_state=42)\n",
    "\n",
    "if X_train_stan.isna().any().any():\n",
    "    print(\"DataFrame contains NaN values\")\n",
    "else:\n",
    "    print(\"DataFrame doesn't contain NaN values\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat X_train_1 and y_train_1 to produce full normalised training database\n",
    "normalised_training_db = pd.concat([X_train_1, y_train_1], axis=1)\n",
    "cor_matrix = normalised_training_db.corr()\n",
    "\n",
    "print(\"Correlation Table:\")\n",
    "print(cor_matrix)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=  0.6455906821963394\n"
     ]
    }
   ],
   "source": [
    "# train using naive bayes \n",
    "\n",
    "# Use Gaussian Naive_Bayes on the train dataset \n",
    "gnb = GaussianNB()\n",
    "unstand_gnb = gnb.fit(X_train_1, y_train_1)\n",
    "\n",
    "# Use Gaussina Naive Bayes to predict for the test dataset \n",
    "y_pred_gnb = unstand_gnb.predict(X_test)\n",
    "\n",
    "stand_gnb = gnb.fit(X_train_stan, y_train_1)\n",
    "y_pred_gnb_stan = stand_gnb.predict(X_test_stan)\n",
    "\n",
    "#delete\n",
    "stand_gnb_split = gnb.fit(X_train, y_train)\n",
    "pred_gnb_split = stand_gnb_split.predict(X_test_1)\n",
    "accuracy = accuracy_score(y_test_1, pred_gnb_split)\n",
    "print(\"accuracy= \", accuracy)   \n",
    "\n",
    "\n",
    "ids = np.arange(1, len(y_pred_gnb) + 1)\n",
    "y_pred_gnb_pd = np.column_stack((ids, y_pred_gnb))\n",
    "np.savetxt(\"y_pred_gnb.csv\", y_pred_gnb_pd, fmt='%d', delimiter=',', header='id,imdb_score_binned', comments='')\n",
    "\n",
    "ids = np.arange(1, len(y_pred_gnb_stan) + 1)\n",
    "y_pred_gnb_pd_stan = np.column_stack((ids, y_pred_gnb_stan))\n",
    "np.savetxt(\"y_pred_gnb_stan.csv\", y_pred_gnb_pd_stan, fmt='%d', delimiter=',', header='id,imdb_score_binned', comments='')\n",
    "\n",
    "# print(\"y_pred_gnb:\\n\", y_pred_gnb_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=  0.6256239600665557\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create kNN classifier\n",
    "k = 3\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "# Train the classifier \n",
    "unstand_knn = knn.fit(X_train_1, y_train_1)\n",
    "stand_knn = knn.fit(X_train_stan, y_train_1)\n",
    "\n",
    "# split \n",
    "stand_knn_split = knn.fit(X_train, y_train)\n",
    "pred_knn_split = stand_knn_split.predict(X_test_1)\n",
    "accuracy = accuracy_score(y_test_1, pred_knn_split)\n",
    "print(\"accuracy= \", accuracy)\n",
    "\n",
    "# Check for NaN values\n",
    "# if X_test.isna().any().any():\n",
    "#     print(\"DataFrame contains NaN values\")\n",
    "# else:\n",
    "#     print(\"DataFrame doesn't contain NaN values\")\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "y_pred_knn_stan = stand_knn.predict(X_test_stan)\n",
    "# np.shape(X_test)\n",
    "\n",
    "ids = np.arange(1, len(y_pred_knn) + 1)\n",
    "y_pred_knn_pd = np.column_stack((ids, y_pred_knn))\n",
    "np.savetxt(\"y_pred_knn.csv\", y_pred_knn_pd, fmt='%d', delimiter=',', header='id,imdb_score_binned', comments='')\n",
    "\n",
    "ids = np.arange(1, len(y_pred_knn_stan) + 1)\n",
    "y_pred_knn_stan_pd = np.column_stack((ids, y_pred_knn_stan))\n",
    "np.savetxt(\"y_pred_stan_knn.csv\", y_pred_knn_stan_pd, fmt='%d', delimiter=',', header='id,imdb_score_binned', comments='')\n",
    "\n",
    "# print(\"y_pred_knn:\\n\", y_pred_knn_pd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame doesn't contain NaN values\n",
      "DataFrame doesn't contain NaN values\n",
      "Columns containing NaN values:\n",
      "Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "np.shape(X_train_stan)\n",
    "\n",
    "if X_train_1.isna().any().any():\n",
    "    print(\"DataFrame contains NaN values\")\n",
    "else:\n",
    "    print(\"DataFrame doesn't contain NaN values\")\n",
    "\n",
    "if X_train_stan.isna().any().any():\n",
    "    print(\"DataFrame contains NaN values\")\n",
    "else:\n",
    "    print(\"DataFrame doesn't contain NaN values\")\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "# Check for NaN values in each column\n",
    "nan_columns = X_train_stan.columns[X_train_stan.isna().any()]\n",
    "\n",
    "# Print the columns containing NaN values\n",
    "print(\"Columns containing NaN values:\")\n",
    "print(nan_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=  0.6938435940099834\n"
     ]
    }
   ],
   "source": [
    "# SVM model \n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df_train.iloc[:,:-1], df_train.iloc[:,-1], test_size=0.2, random_state=42)\n",
    "\n",
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "# Train the SVM classifier\n",
    "clf.fit(X_train_stan, y_train_1)\n",
    "\n",
    "\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred_SVM_stan= clf.predict(X_test_stan)\n",
    "\n",
    "#split data \n",
    "stand_SVM_split = clf.fit(X_train, y_train)\n",
    "pred_SVM_split = stand_SVM_split.predict(X_test_1)\n",
    "accuracy = accuracy_score(y_test_1, pred_SVM_split)\n",
    "print(\"accuracy= \", accuracy)\n",
    "\n",
    "ids = np.arange(1, len(y_pred_SVM_stan) + 1)\n",
    "y_pred_SVM_stan_pd = np.column_stack((ids, y_pred_SVM_stan))\n",
    "np.savetxt(\"y_pred_stan_SVM.csv\", y_pred_SVM_stan_pd, fmt='%d', delimiter=',', header='id,imdb_score_binned', comments='')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 21.33074990\n",
      "Iteration 2, loss = 19.18196474\n",
      "Iteration 3, loss = 19.89457210\n",
      "Iteration 4, loss = 20.83481888\n",
      "Iteration 5, loss = 19.46401310\n",
      "Iteration 6, loss = 20.20186126\n",
      "Iteration 7, loss = 19.03170567\n",
      "Iteration 8, loss = 18.03943925\n",
      "Iteration 9, loss = 17.36843943\n",
      "Iteration 10, loss = 19.39670048\n",
      "Iteration 11, loss = 21.08227460\n",
      "Iteration 12, loss = 20.02564866\n",
      "Iteration 13, loss = 19.34344602\n",
      "Iteration 14, loss = 18.12542246\n",
      "Iteration 15, loss = 20.61165903\n",
      "Iteration 16, loss = 18.72882189\n",
      "Iteration 17, loss = 18.83811364\n",
      "Iteration 18, loss = 24.15864914\n",
      "Iteration 19, loss = 17.38839726\n",
      "Iteration 20, loss = 16.69627769\n",
      "Iteration 21, loss = 20.77169381\n",
      "Iteration 22, loss = 19.19437211\n",
      "Iteration 23, loss = 17.70111285\n",
      "Iteration 24, loss = 17.67091304\n",
      "Iteration 25, loss = 17.23835036\n",
      "Iteration 26, loss = 18.10311354\n",
      "Iteration 27, loss = 18.19530350\n",
      "Iteration 28, loss = 17.99783768\n",
      "Iteration 29, loss = 17.89825562\n",
      "Iteration 30, loss = 21.26046600\n",
      "Iteration 31, loss = 17.59118726\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.04648604\n",
      "Iteration 2, loss = 0.60148142\n",
      "Iteration 3, loss = 0.06090164\n",
      "Iteration 4, loss = 0.01037631\n",
      "Iteration 5, loss = 0.00495326\n",
      "Iteration 6, loss = 0.00355216\n",
      "Iteration 7, loss = 0.00284807\n",
      "Iteration 8, loss = 0.00241593\n",
      "Iteration 9, loss = 0.00210211\n",
      "Iteration 10, loss = 0.00185613\n",
      "Iteration 11, loss = 0.00166619\n",
      "Iteration 12, loss = 0.00150679\n",
      "Iteration 13, loss = 0.00137183\n",
      "Iteration 14, loss = 0.00125636\n",
      "Iteration 15, loss = 0.00115948\n",
      "Iteration 16, loss = 0.00107659\n",
      "Iteration 17, loss = 0.00100131\n",
      "Iteration 18, loss = 0.00093634\n",
      "Iteration 19, loss = 0.00087553\n",
      "Iteration 20, loss = 0.00082399\n",
      "Iteration 21, loss = 0.00077838\n",
      "Iteration 22, loss = 0.00073714\n",
      "Iteration 23, loss = 0.00069936\n",
      "Iteration 24, loss = 0.00066578\n",
      "Iteration 25, loss = 0.00063434\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.81120695\n",
      "Iteration 2, loss = 0.49776046\n",
      "Iteration 3, loss = 0.04203777\n",
      "Iteration 4, loss = 0.01137919\n",
      "Iteration 5, loss = 0.00562042\n",
      "Iteration 6, loss = 0.00385075\n",
      "Iteration 7, loss = 0.00298306\n",
      "Iteration 8, loss = 0.00248231\n",
      "Iteration 9, loss = 0.00214111\n",
      "Iteration 10, loss = 0.00189771\n",
      "Iteration 11, loss = 0.00169501\n",
      "Iteration 12, loss = 0.00153379\n",
      "Iteration 13, loss = 0.00138843\n",
      "Iteration 14, loss = 0.00126342\n",
      "Iteration 15, loss = 0.00116160\n",
      "Iteration 16, loss = 0.00107375\n",
      "Iteration 17, loss = 0.00099753\n",
      "Iteration 18, loss = 0.00092920\n",
      "Iteration 19, loss = 0.00086961\n",
      "Iteration 20, loss = 0.00081661\n",
      "Iteration 21, loss = 0.00077043\n",
      "Iteration 22, loss = 0.00072904\n",
      "Iteration 23, loss = 0.00069225\n",
      "Iteration 24, loss = 0.00065741\n",
      "Iteration 25, loss = 0.00062528\n",
      "Iteration 26, loss = 0.00059627\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "accuracy=  0.41098169717138106\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Define and train the neural network classifier\n",
    "clf_MLP = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, alpha=0.0001,\n",
    "                    solver='adam', verbose=10, random_state=42, tol=0.0001)\n",
    "unstand_clf = clf_MLP.fit(X_train_1, y_train_1)\n",
    "stand_clf = clf_MLP.fit(X_train_stan, y_train_1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_MLP = unstand_clf.predict(X_test)\n",
    "y_pred_MLP_stan = stand_clf.predict(X_test_stan)\n",
    "\n",
    "ids = np.arange(1, len(y_pred_MLP) + 1)\n",
    "y_pred_MLP_pd = np.column_stack((ids, y_pred_MLP))\n",
    "np.savetxt(\"y_pred_MLP.csv\", y_pred_MLP_pd, fmt='%d', delimiter=',', header='id,imdb_score_binned', comments='')\n",
    "\n",
    "#split data \n",
    "stand_MLP_split = clf_MLP.fit(X_train, y_train)\n",
    "pred_MLP_split = stand_MLP_split.predict(X_test_1)\n",
    "accuracy = accuracy_score(y_test_1, pred_MLP_split)\n",
    "print(\"accuracy= \", accuracy)\n",
    "\n",
    "\n",
    "ids = np.arange(1, len(y_pred_MLP_stan) + 1)\n",
    "y_pred_MLP_stan_pd = np.column_stack((ids, y_pred_MLP_stan))\n",
    "np.savetxt(\"y_pred_stan_MLP.csv\", y_pred_MLP_stan_pd, fmt='%d', delimiter=',', header='id,imdb_score_binned', comments='')\n",
    "\n",
    "# print(\"y_pred_MLP:\\n\", y_pred_MLP_stan_pd)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
